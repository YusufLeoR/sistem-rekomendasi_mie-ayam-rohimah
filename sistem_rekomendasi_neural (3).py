# -*- coding: utf-8 -*-
"""sistem_rekomendasi_neural.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sjd4PPodFe0wN1L_S1oWPeNHkZYmBBYk

## Import Library
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df_toko = pd.read_csv('/content/drive/MyDrive/Datathon/Datathon_mie ayam rohimah - toko_generated.csv')
df_user = pd.read_csv('/content/drive/MyDrive/Datathon/Datathon_mie ayam rohimah - user_generated.csv')
df_video = pd.read_csv('/content/drive/MyDrive/Datathon/Datathon_mie ayam rohimah - video_generated.csv')

"""**Load Dataset**"""

df_toko.head()

df_user.head()

df_video.head()

"""**info dataset**"""

df_toko.info()

df_user.info()

df_video.info()

"""#Preprocessing"""

df_video = df_video.rename(columns={"kategori": "kategori_video"})

df_toko = df_toko.rename(columns={"kategori": "kategori_toko"})

#Gabungan user dan video berdasarkan video_id:
df_merged = df_user.merge(
    df_video,
    on="video_id",
    suffixes=("", "_video")
)
print("\n===== Merge User + Video =====")
df_merged.head()

#Gabungkan hasil tadi dengan Toko berdasarkan toko_id
df_merged = df_merged.merge(
    df_toko,
    on="toko_id",
    suffixes=("", "_toko")
)
print("\n===== Merge Lengkap (User + Video + Toko) =====")
df_merged.head()

df_merged.info()

df_clean = df_merged[[
    "user_id",
    "video_id",
    "toko_id",
    "liked",
    "distance",
    "latitude",
    "longitude",
    "kategori_video",
    "likes_video_video",
    "duration",
    "lokasi",
    "kategori_toko",
    "rating"
]].copy()

df_clean = df_clean.rename(columns={
    "likes_video_video": "likes_video",
    "lokasi_toko": "lokasi",
    "kategori_toko_toko": "kategori_toko",
    "rating_toko": "rating"
})

print("Kolom dataframe bersih:")
df_clean.columns

print("\nPreview 5 baris data:")
df_clean.head()

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

le_kategori_video = LabelEncoder()
le_kategori_toko = LabelEncoder()
le_lokasi = LabelEncoder()

# Encode kategori video
df_clean["kategori_video_enc"] = le_kategori_video.fit_transform(df_clean["kategori_video"])
# Encode kategori toko
df_clean["kategori_toko_enc"] = le_kategori_toko.fit_transform(df_clean["kategori_toko"])
# Encode lokasi
df_clean["lokasi_enc"] = le_lokasi.fit_transform(df_clean["lokasi"])

scaler = MinMaxScaler()

# Kolom numeric yang mau dinormalisasi
numeric_cols = ["distance", "latitude", "longitude", "likes_video", "duration", "rating"]
# Transform
df_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])

print("\nKolom setelah encoding dan normalisasi:")
df_clean[["kategori_video", "kategori_video_enc", "kategori_toko", "kategori_toko_enc", "lokasi", "lokasi_enc"]].head()

print("\nKolom numeric setelah normalisasi:")
df_clean[numeric_cols].head()

"""#Splitting"""

# Target
y = df_clean["liked"]

# Feature columns
feature_cols = [
    "user_id",
    "video_id",
    "toko_id",
    "distance",
    "latitude",
    "longitude",
    "likes_video",
    "duration",
    "rating",
    "kategori_video_enc",
    "kategori_toko_enc",
    "lokasi_enc"
]

X = df_clean[feature_cols]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train size:", len(X_train))
print("Test size:", len(X_test))

"""#Model Preparation"""

# Convert DataFrame ke dictionary of arrays
X_train_dict = {col: X_train[col].values for col in X_train.columns}
X_test_dict = {col: X_test[col].values for col in X_test.columns}

import tensorflow as tf
from tensorflow.keras import layers, Model, Input

# Embedding Inputs
user_input = Input(shape=(1,), name="user_id")
video_input = Input(shape=(1,), name="video_id")
toko_input = Input(shape=(1,), name="toko_id")

# Categorical Encoded Inputs
kategori_video_input = Input(shape=(1,), name="kategori_video_enc")
kategori_toko_input = Input(shape=(1,), name="kategori_toko_enc")
lokasi_input = Input(shape=(1,), name="lokasi_enc")

# Numeric Inputs
distance_input = Input(shape=(1,), name="distance")
latitude_input = Input(shape=(1,), name="latitude")
longitude_input = Input(shape=(1,), name="longitude")
likes_video_input = Input(shape=(1,), name="likes_video")
duration_input = Input(shape=(1,), name="duration")
rating_input = Input(shape=(1,), name="rating")

embedding_dim = 8

user_emb = layers.Embedding(input_dim=10000, output_dim=embedding_dim)(user_input)
video_emb = layers.Embedding(input_dim=10000, output_dim=embedding_dim)(video_input)
toko_emb = layers.Embedding(input_dim=1000, output_dim=embedding_dim)(toko_input)

kategori_video_emb = layers.Embedding(input_dim=10, output_dim=4)(kategori_video_input)
kategori_toko_emb = layers.Embedding(input_dim=10, output_dim=4)(kategori_toko_input)
lokasi_emb = layers.Embedding(input_dim=10, output_dim=4)(lokasi_input)

user_emb_flat = layers.Flatten()(user_emb)
video_emb_flat = layers.Flatten()(video_emb)
toko_emb_flat = layers.Flatten()(toko_emb)
kategori_video_emb_flat = layers.Flatten()(kategori_video_emb)
kategori_toko_emb_flat = layers.Flatten()(kategori_toko_emb)
lokasi_emb_flat = layers.Flatten()(lokasi_emb)

concat_layer = layers.Concatenate()([
    user_emb_flat,
    video_emb_flat,
    toko_emb_flat,
    kategori_video_emb_flat,
    kategori_toko_emb_flat,
    lokasi_emb_flat,
    distance_input,
    latitude_input,
    longitude_input,
    likes_video_input,
    duration_input,
    rating_input
])

dense1 = layers.Dense(64, activation="relu")(concat_layer)
dense2 = layers.Dense(32, activation="relu")(dense1)
output = layers.Dense(1, activation="sigmoid")(dense2)

model = Model(
    inputs=[
        user_input,
        video_input,
        toko_input,
        kategori_video_input,
        kategori_toko_input,
        lokasi_input,
        distance_input,
        latitude_input,
        longitude_input,
        likes_video_input,
        duration_input,
        rating_input
    ],
    outputs=output
)

model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["AUC"]
)

print("\nData types di X_train_dict:")
for k, v in X_train_dict.items():
    print(f"{k}: {v.dtype}")

# Convert embeddings columns ke int32
embed_cols = ["user_id", "video_id", "toko_id", "kategori_video_enc", "kategori_toko_enc", "lokasi_enc"]
for col in embed_cols:
    X_train_dict[col] = X_train_dict[col].astype("int32")
    X_test_dict[col] = X_test_dict[col].astype("int32")

# Convert numeric columns ke float32
numeric_cols = ["distance", "latitude", "longitude", "likes_video", "duration", "rating"]
for col in numeric_cols:
    X_train_dict[col] = X_train_dict[col].astype("float32")
    X_test_dict[col] = X_test_dict[col].astype("float32")

print("\nSetelah konversi dtype:")
for k, v in X_train_dict.items():
    print(f"{k}: {v.dtype}")

"""#train model"""

history = model.fit(
    X_train_dict,
    y_train,
    validation_data=(X_test_dict, y_test),
    epochs=20,
    batch_size=512
)

"""#Evaluasi"""

results = model.evaluate(X_test_dict, y_test, verbose=2)
print("Test Loss:", results[0])
print("Test AUC :", results[1])

# Prediksi probabilitas
y_pred_proba = model.predict(X_test_dict).flatten()

# Thresholding: jika prob > 0.5 dianggap "like"
y_pred_label = (y_pred_proba >= 0.5).astype(int)

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_label)
print("Confusion Matrix:\n", cm)

# Plot confusion matrix
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
report = classification_report(y_test, y_pred_label)
print("\nClassification Report:\n", report)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, color='blue', label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0,1], [0,1], color='gray', linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""#Inference"""

def recommend_top_n(
    model,
    target_user_id,
    df_video,
    df_toko,
    le_kategori_video,
    le_kategori_toko,
    le_lokasi,
    scaler,
    top_n=5
):
    import numpy as np
    import pandas as pd

    # Siapkan video
    candidates = df_video.copy()
    candidates["user_id"] = target_user_id

    # Join ke toko
    candidates = candidates.merge(
        df_toko[["toko_id", "kategori_toko", "lokasi", "rating"]],
        on="toko_id",
        how="left"
    )

    # Encode kategori video
    candidates["kategori_video_enc"] = le_kategori_video.transform(candidates["kategori_video"])

    # Encode kategori toko
    candidates["kategori_toko_enc"] = le_kategori_toko.transform(candidates["kategori_toko"])

    # Encode lokasi
    candidates["lokasi_enc"] = le_lokasi.transform(candidates["lokasi"])

    # Dummy distance, lat, long
    candidates["distance"] = 0.0
    candidates["latitude"] = 0.0
    candidates["longitude"] = 0.0

    # Normalisasi numeric
    numeric_cols = ["distance", "latitude", "longitude", "likes_video", "duration", "rating"]
    candidates[numeric_cols] = scaler.transform(candidates[numeric_cols])

    # Siapkan input dictionary
    X_input = {}
    input_names = [
        "user_id",
        "video_id",
        "toko_id",
        "kategori_video_enc",
        "kategori_toko_enc",
        "lokasi_enc",
        "distance",
        "latitude",
        "longitude",
        "likes_video",
        "duration",
        "rating",
    ]

    for col in input_names:
        arr = candidates[col].to_numpy().astype(np.float32 if col in numeric_cols else np.int32)
        X_input[col] = arr.reshape(-1, 1)

    # Debug print
    for k, v in X_input.items():
        print(f"{k}: shape={v.shape}, dtype={v.dtype}")

    # Siapkan input list sesuai urutan Input() di model
    X_input_list = [
        X_input["user_id"],
        X_input["video_id"],
        X_input["toko_id"],
        X_input["kategori_video_enc"],
        X_input["kategori_toko_enc"],
        X_input["lokasi_enc"],
        X_input["distance"],
        X_input["latitude"],
        X_input["longitude"],
        X_input["likes_video"],
        X_input["duration"],
        X_input["rating"],
    ]

    # Prediksi
    preds = model.predict(X_input_list, verbose=0).flatten()
    candidates["prob_like"] = preds

    # Top N
    top_n_df = candidates.sort_values("prob_like", ascending=False).head(top_n)

    return top_n_df[["video_id", "toko_id", "kategori_video", "kategori_toko", "lokasi", "prob_like"]]

print(df_video.columns)
print(df_toko.columns)

model.summary()

# Ambil 1 baris saja dari df_video
sample_df = df_video.head(1).copy()
sample_df["user_id"] = 103

# Join toko
sample_df = sample_df.merge(
    df_toko[["toko_id", "kategori_toko", "lokasi", "rating"]],
    on="toko_id",
    how="left"
)

# Encode
sample_df["kategori_video_enc"] = le_kategori_video.transform(sample_df["kategori_video"])
sample_df["kategori_toko_enc"] = le_kategori_toko.transform(sample_df["kategori_toko"])
sample_df["lokasi_enc"] = le_lokasi.transform(sample_df["lokasi"])

# Dummy numeric
sample_df["distance"] = 0.0
sample_df["latitude"] = 0.0
sample_df["longitude"] = 0.0

# Normalisasi
numeric_cols = ["distance", "latitude", "longitude", "likes_video", "duration", "rating"]
sample_df[numeric_cols] = scaler.transform(sample_df[numeric_cols])

# Build X_input
X_sample = {}
input_names = [
    "user_id",
    "video_id",
    "toko_id",
    "kategori_video_enc",
    "kategori_toko_enc",
    "lokasi_enc",
    "distance",
    "latitude",
    "longitude",
    "likes_video",
    "duration",
    "rating",
]

for col in input_names:
    arr = sample_df[col].to_numpy().astype(np.float32 if col in numeric_cols else np.int32)
    X_sample[col] = arr.reshape(-1, 1)

for k, v in X_sample.items():
    print(f"{k}: shape={v.shape}, dtype={v.dtype}, has_nan={np.isnan(v).any()}")
    print(v[:5])

# Predict
X_sample_list = [
    X_sample["user_id"],
    X_sample["video_id"],
    X_sample["toko_id"],
    X_sample["kategori_video_enc"],
    X_sample["kategori_toko_enc"],
    X_sample["lokasi_enc"],
    X_sample["distance"],
    X_sample["latitude"],
    X_sample["longitude"],
    X_sample["likes_video"],
    X_sample["duration"],
    X_sample["rating"],
]

pred = model.predict(X_sample_list)
print("Prediksi sukses:", pred)

top_recommendations = recommend_top_n(
    model=model,
    target_user_id=103,
    df_video=df_video,
    df_toko=df_toko,
    le_kategori_video=le_kategori_video,
    le_kategori_toko=le_kategori_toko,
    le_lokasi=le_lokasi,
    scaler=scaler,
    top_n=5
)

print(top_recommendations)

model.save('rekomendasi_model.h5')

import joblib
joblib.dump(le_kategori_video, "le_kategori_video.pkl")
joblib.dump(le_kategori_toko, "le_kategori_toko.pkl")
joblib.dump(le_lokasi, "le_lokasi.pkl")
joblib.dump(scaler, "scaler.pkl")